Перевод: Часть 1: Основы обучения с подкреплением и глубокое Q-обучение

Патрик Лобер – популярный преподаватель Python, и в этом курсе он научит вас, как обучить искусственный интеллект играть в игру "Змейка" с использованием обучения с подкреплением. Привет, ребята, сегодня у меня для вас очень интересный проект: мы собираемся создать ИИ, который сам научится играть в "Змейку", и мы построим все с нуля. Итак, мы начнем с создания игры с помощью Pygame, а затем мы построим агента и алгоритм глубокого обучения с помощью PyTorch. Я также научу вас основам обучения с подкреплением, которые нам нужно понять, чтобы понять, как все это работает. Так что я думаю, что это будет довольно круто, и теперь, прежде чем мы начнем, позвольте мне показать вам финальный проект. Итак, я могу запустить скрипт, сказав:

python agents.pi

Теперь это начнет обучение нашего агента.

И здесь мы видим нашу игру, а затем здесь я также строю графики очков, а затем среднего балла. И теперь позвольте мне также запустить секундомер, чтобы вы могли видеть, что все это происходит вживую. И теперь на данный момент наша змейка совершенно ничего не знает об игре. Она только осознает окружающую среду и пытается делать более или менее случайные ходы. Но с каждым ходом и особенно с каждой игрой она учится все больше и больше, а затем знает, как играть в игру, и она должна становиться все лучше и лучше.

Так что в первые несколько игр вы не увидите больших улучшений, но не волнуйтесь, это абсолютно нормально. Я могу сказать вам, что требуется около 80-100 игр, пока наш ИИ не разработает хорошую игровую стратегию, и это займет около 10 минут. Также вам не нужен графический процессор для этого, поэтому все это обучение может происходить на этом центральном процессоре, это совершенно нормально. Хорошо, позвольте мне немного ускорить это.

 
 

Итак, прошло около 10 минут, и мы примерно на 90-й игре, я думаю. И теперь мы ясно видим, что наша змейка знает, что ей следует делать. Так что она более или менее идет прямо к еде и старается не задеть границы. Так что она не идеальна на данный момент, но мы видим, что она становится все лучше и лучше. Так что мы также видим, что средний балл здесь увеличивается, и теперь лучший балл на данный момент – ... И, честно говоря, для меня это супер-захватывающе. Так что если вы представите, что в начале наша змейка ничего не знала об игре, а теперь, с небольшим количеством математики за кулисами, она явно следует стратегии. Так что это просто супер-круто, не так ли? Хорошо, позвольте мне немного ускорить это.

 

Итак, через 12 минут наша змейка становится все лучше и лучше, так что я думаю, что вы можете ясно видеть, что наш алгоритм работает. Так что теперь позвольте мне остановить это, а затем начнем с теории. Итак, я разделю серию на четыре части. В этом первом видео мы немного узнаем о теории обучения с подкреплением. Во второй части мы реализуем саму игру, или также называемую средой, здесь с помощью Pygame. Затем мы реализуем агента, так что я скажу вам, что это значит через секунду, и в последней части мы реализуем саму модель с помощью PyTorch.

Итак, начнем с небольшой теории об обучении с подкреплением. Итак, это определение из Википедии: обучение с подкреплением – это область машинного обучения, которая занимается тем, как программные агенты должны предпринимать действия в среде, чтобы максимизировать понятие совокупного вознаграждения. Так что это может звучать немного сложно. Другими словами, мы можем также сказать, что обучение с подкреплением – это обучение программного агента тому, как вести себя в среде, говоря ему, насколько хорошо он справляется.

Итак, что мы должны помнить здесь, это то, что у нас есть шанс, так что это в основном наш компьютерный игрок, затем у нас есть окружающая среда, так что это наша игра в этом случае, а затем мы даем агенту вознаграждение, так что с этим мы говорим ему, насколько хорошо он справляется, а затем, основываясь на этом вознаграждении, он должен попытаться найти наилучшее следующее действие. Так что да, это обучение с подкреплением.

И для обучения агента существует множество различных подходов, и не все из них включают глубокое обучение, но в нашем случае мы используем глубокое обучение, и это также называется глубоким Q-обучением. Итак, этот подход расширяет обучение с подкреплением, используя глубокую нейронную сеть для прогнозирования действий, и это то, что мы собираемся использовать в этом уроке.

Хорошо, позвольте мне показать вам краткий обзор того, как я организовал код. Итак, как я уже сказал, у нас четыре части, поэтому в следующей части мы реализуем игру с помощью PiGame. Затем мы реализуем агента, а затем мы реализуем модель с помощью PiTorch. Итак, наша игра должна быть разработана так, чтобы у нас был игровой цикл, и затем с каждым игровым циклом мы выполняем игровой шаг, который получает действие, а затем он делает шаг, так что он перемещает змейку, и затем после этого хода он возвращает текущее вознаграждение и если игра окончена или нет, а затем также текущий счет. Затем у нас есть агент, и агент в основном объединяет все вместе, поэтому поэтому он должен знать об игре и он также знает о модели, поэтому мы храним оба их в нашем агенте и затем мы реализуем цикл обучения, так что это примерно то, что мы должны делать.

Итак, на основе игры мы должны вычислить состояние, а затем на основе состояния мы вычисляем следующее действие, и это включает в себя вызов model.predict, а затем с этим новым действием мы делаем следующий игровой шаг, и затем, как я уже сказал, мы получаем вознаграждение, состояние окончания игры и счет, и теперь с этой информацией мы вычисляем новое состояние, а затем мы запоминаем все это, поэтому мы храним новое состояние и старое состояние, состояние окончания игры и счет, и с этим мы затем тренируем нашу модель.

Итак, для модели я называю это linearQNet, так что это не слишком сложно, это просто прямая нейронная сеть с несколькими линейными слоями, и она должна иметь эту информацию, так что новое состояние и старое состояние, и затем мы можем обучить модель, и мы можем вызвать model.predict, и затем это даст нам следующее действие, так что да, это краткий обзор того, как должен выглядеть код, и теперь давайте поговорим о некоторых из этих переменных более подробно, например, о действии или состоянии или вознаграждении. Итак, давайте начнем с вознаграждения, так что это довольно легко, поэтому всякий раз, когда наша змейка съедает еду, мы даем ей плюс 10 вознаграждения, когда игра окончена, так что когда мы умираем, тогда мы получаем -10, а для всего остального мы просто остаемся на нуле. Так что это довольно просто.

Затем у нас есть действие, так что действие определяет наш следующий ход, так что мы могли бы подумать, что у нас есть четыре различных действия, так что влево, вправо, вверх и вниз, но если мы разработаем это таким образом, то, например, что может произойти, так это то, что если мы идем вправо, то мы можем предпринять действие влево, и тогда мы немедленно умираем, так что это в основном поворот на 180 градусов, поэтому мы не позволяем этого. Так что лучшим подходом к разработке действия является использование только трех разных чисел, и теперь это зависит от текущего направления, так что 1 0 0 означает, что мы остаемся в текущем направлении, так что мы идем прямо, так что это означает, что если мы идем вправо, то мы остаемся вправо, если мы идем влево, то мы идем влево и так далее.

Тогда, если у нас есть 0 1 0, это означает, что мы делаем поворот направо, и опять же это зависит от текущего направления, так что если мы идем вправо и делаем поворот направо, то мы идем вниз дальше, если мы идем вниз и делаем поворот направо, то мы идем влево, и затем опять же мы пойдем вверх, так что это поворот направо, а поворот налево – наоборот, так что если мы идем влево и делаем поворот налево, то мы идем вниз и так далее. Так что с этим подходом мы не можем сделать поворот на 180 градусов, и нам также нужно предсказать только три разных состояния, так что это немного облегчит задачу для нашей модели. Итак, теперь у нас есть вознаграждение и действие, затем нам также нужно вычислить состояние, и состояние означает, что мы должны сообщить нашей змейке некоторую информацию об игре, о которой она знает, поэтому ей нужно знать об окружающей среде, и в этом случае наше состояние имеет 11 значений, так что оно имеет информацию о том, если опасность прямо или если она впереди, если опасность справа или если опасность слева, затем оно имеет текущее направление, так что направление влево, вправо, вверх и вниз, и затем оно имеет информацию о том, находится ли еда слева или справа или вверх или вниз, и все это булевы значения.

Итак, позвольте мне показать вам реальный пример, так что в этом случае, если мы идем вправо, и наша еда находится здесь, то мы видим, что опасность прямо, справа и слева – ничего из этого не верно. Итак, например, если наша змейка находится здесь, в этом конце, и она все еще идет вправо, то опасность прямо будет единицей, так что это опять же также зависит от текущего направления. Например, если мы движемся вверх в этом углу здесь, то опасность справа будет единицей.

Тогда для этих направлений только одно из них равно единице, а остальное всегда равно нулю. Так что в этом случае у нас опасность справа установлена в единицу, и тогда для этого в нашем случае наша еда находится справа от змейки, а также внизу от змейки, так что еда справа – единица, а еда внизу – единица. Хорошо, так что теперь, имея состояние и действие, мы можем спроектировать нашу модель, так что это просто прямая нейронная сеть со входным слоем, скрытым слоем и выходным слоем, и для входа она получает состояние, так что, как я уже сказал, у нас 11 разных чисел в нашем состоянии, 11 разных булевых значений, ноль или единица, так что нам нужен этот размер 11 в начале. Затем мы можем выбрать скрытый размер, и для вывода нам нужно три выхода, потому что тогда мы предсказываем действие, так что это могут быть некоторые числа, и они не обязательно должны быть вероятностями, так что здесь у нас могут быть необработанные числа, и затем мы просто выбираем максимум, так что, например, если мы возьмем 1 0 0, и если мы вернемся назад, то мы увидим, что это было бы действием прямо, так что сохранить текущее направление. Так что да, так выглядит наша модель.

И конечно, теперь мы должны обучить модель, так что для этого давайте немного поговорим об этом глубоком Q-обучении, так что Q означает это Q-значение, и это означает качество действия, так что это то, что мы хотим улучшить, так что каждое действие должно улучшать качество змейки. Итак, мы начинаем с инициализации Q-значения, так что в этом случае мы инициализируем нашу модель с некоторыми случайными параметрами. Затем мы выбираем действие, вызывая model.predict(state), и мы также иногда выбираем случайный ход. Мы делаем это особенно в начале, когда мы еще многого не знаем об игре, и затем позже мы должны пойти на компромисс, когда мы больше не хотим делать случайный ход и только вызывать model.predict. И это также называется компромиссом между исследованием и эксплуатацией, так что это станет яснее позже, когда мы будем делать фактическое кодирование. Итак, затем с этим новым действием мы выполняем это действие, поэтому мы выполняем следующий ход, а затем мы измеряем вознаграждение, и с этой информацией мы можем обновить наше Q-значение, а затем обучить модель, а затем мы повторяем этот шаг, так что это итеративный цикл обучения. Итак, теперь, чтобы обучить модель, как всегда, нам нужно иметь какую-то функцию потерь, которую мы хотим оптимизировать или минимизировать.

Итак, для функции потерь нам нужно взглянуть на немного математики, и для этого я хочу представить вам так называемое уравнение Беллмана. Так что это может выглядеть страшно, так что не бойтесь, я объясню все, и на самом деле это не так уж и сложно, когда мы понимаем это, а затем кодируем это позже. Итак, что мы хотим сделать здесь, нам нужно обновить Q-значение, как я уже сказал здесь, так что согласно уравнению Беллмана, новое Q-значение вычисляется так: у нас есть текущее Q-значение плюс скорость обучения, а затем у нас есть вознаграждение за выполнение этого действия в этом состоянии плюс гамма-параметр, который называется коэффициентом дисконтирования, так что не беспокойтесь об этом, я также покажу это позже в коде, и затем мы берем максимальное ожидаемое будущее вознаграждение, учитывая новое состояние и все возможные действия в этом новом состоянии. Так что да, это выглядит страшно, но я упрощу это для вас, и тогда это на самом деле не так уж и сложно. Старое Q-значение – это model.predict со state 0.

Итак, если мы вернемся к этому обзору, то в первый раз, когда мы говорим "получить состояние из игры", это наше состояние 0, и затем после того, как мы сделали этот шаг, мы снова измеряем или вычисляем следующее состояние, так что это тогда наше состояние 1. Так что с этой информацией опять же наша первая очередь – это просто model.predict со старым состоянием, а затем новая очередь – это вознаграждение плюс наше гамма-значение, умноженное на максимальное значение Q-состояния, так что опять же это model.predict, но на этот раз мы берем состояние 1. И затем с этой двумя информацими наша потеря – это просто q_new минус q в квадрате, и да, это не что иное, как среднеквадратическая ошибка, так что это очень простая ошибка, о которой мы должны уже знать, и затем это то, что мы должны использовать в нашей оптимизации. Так что да, это то, что мы собираемся использовать, поэтому мы должны реализовать все эти три класса, и в следующем видео мы начнем с реализации игры.

Перевод: Часть 2: Настройка среды и реализация игры "Змейка"

В прошлой части я показал вам всю необходимую теорию, которую нам нужно знать, чтобы начать работу с глубоким Q-обучением, и теперь мы начинаем реализовывать все части. Итак, как я уже сказал, нам нужна игра, то есть среда, затем нам нужен агент, и нам нужна модель. Итак, в этой части мы начинаем с реализации игры, и для этого мы используем PyTorch.

Итак, позвольте мне на самом деле начать с создания среды, и мы установим все необходимые зависимости, которые нам нужны. Итак, в этом случае я использую Conda для управления средами, и если вы не знаете, как использовать Conda, то у меня есть учебник для вас, на который я дам ссылку здесь, но да, если вы не хотите использовать Connor, вы также можете просто использовать нормальную виртуальную среду, но я рекомендую использовать виртуальную среду. И теперь давайте создадим виртуальную среду с помощью conda create -n и затем дадим ей имя, например, pygame_env, и я также скажу, что хочу python равным 3.7.

Хорошо, теперь это было создано, поэтому теперь мы хотим активировать это с помощью conda activate и затем pygame_env и нажать Enter, и тогда мы увидим имя среды в начале, так что это означает, что мы активировали это успешно, и теперь мы можем начать установку всего, что нам нужно. Итак, первое, что мы хотим установить, это Pygame для нашей игры, поэтому pip install pygame и нажать Enter.

Итак, это сделано. Следующее, что нам нужно, это PyTorch для нашей модели позже, поэтому для этого мы можем перейти на официальную домашнюю страницу и на install, а затем здесь вы можете выбрать свою операционную систему, поэтому я использую Mac, и я на самом деле я хочу сказать pip install, и нам не нужна поддержка CUDA, поэтому только центрального процессора достаточно, и нам не нужен torch audio, потому что мы не работаем с аудиофайлами, поэтому мы можем просто взять этот pip install torch torchvision и затем вставить его сюда и нажать Enter, и теперь это установит PyTorch и все зависимости.

Хорошо, это сделано, и затем нам нужно еще две вещи для построения графиков позже, поэтому для этого я говорю pip install matplotlib, и нам также нужен iPython и затем нажать Enter.

Хорошо, это тоже было успешно, и теперь у нас есть все, что нам нужно. Итак, теперь мы можем начать реализовывать все коды, и в качестве отправной точки я хочу взять код из другого руководства, которое я сделал, поэтому вы можете найти это на Github и затем на моей учетной записи и затем в репозитории python fun.

И здесь у меня на самом деле есть две игры "Змейка", поэтому, и затем нам нужна эта одна snake_pygame и загрузите это, так что вы можете сделать это, и я уже сделал это, и у меня это здесь, так что если мы откроем редактор здесь, я использую Visual Studio Code, тогда мы увидим, что у нас есть именно эти два файла.

И затем, первое, что я хочу сделать, это запустить этот файл и проверить, действительно ли это работает. Так что прямо сейчас это просто нормальная игра "Змейка", которой вы должны управлять сами, поэтому вы должны использовать клавиши со стрелками. Итак, давайте скажем python snake_game.py, и тогда давайте надеяться, что это работает, так что да, теперь я могу управлять змейкой, и я надеюсь, что я смогу съесть еду, да.

И теперь, если я коснусь границы, то игра окончена. Так что это работает, наша среда настроена, и теперь мы можем начать реализовывать наш код, поэтому мы можем изменить это, чтобы мы могли использовать это как игру, управляемую ИИ. Итак, позвольте мне показать вам обзор с прошлого раза, так что в прошлый раз я сказал вам, что нам нужен игровой шаг в нашей игре, и это получает действие, и на основе этого действия мы затем совершаем ход, и затем мы должны вернуть вознаграждение, состояние окончания игры и текущий счет.

Итак, сначала давайте запишем все вещи, которые нам нужно изменить здесь. Итак, во-первых, мы хотим иметь функцию reset, поэтому после каждой игры наш агент должен иметь возможность сбросить игру и начать новую игру. Затем нам нужно реализовать вознаграждение, которое получает наш агент. Затем нам нужно изменить функцию play, чтобы она принимала действие и затем возвращала или вычисляла направление.

Затем мы также хотим отслеживать текущий кадр или давайте назовем это игровой итерацией. И для более поздней части нам также нужно иметь изменение в функции if is_collision для проверки, является ли это столкновением. Итак, сначала позвольте мне быстро пройтись по этому коду, так что что мы делаем здесь, это мы используем Pygame, затем для направления мы используем enum.

Затем для точки мы используем именованный кортеж. И затем здесь я создал класс SnakeGame и здесь мы инициализируем вещи, которые нам нужны для игры. Так что здесь мы инициализируем состояние игры, например, для змейки мы используем список с тремя начальными значениями, и голова всегда находится в начале этого списка. Затем мы отслеживаем счет, и здесь у нас есть вспомогательная функция для размещения еды, и да, и у нас уже есть функция, которая называется play_step.

И затем, если мы опустимся в самый конец, так что здесь у нас есть наш игровой цикл, так что пока это правда, мы делаем игру или игровой шаг, и мы получаем состояние окончания игры и счет. Так что эта функция play является самой важной, так что здесь мы в первую очередь, прямо сейчас, получаем пользовательский ввод, то есть клавишу, которую мы нажимаем. Затем мы вычисляем ход на основе этой клавиши, и затем мы обновляем нашу змейку и проверяем, закончилась ли игра, и если мы можем продолжать, мы размещаем новую еду или проверяем, можем ли мы съесть еду, и мы обновляем наш пользовательский интерфейс с помощью этой вспомогательной функции update_ui. Затем здесь у нас есть эта вспомогательная функция is_collision, где мы проверяем, либо мы коснулись границы, либо мы врезались в себя.

И затем у нас также есть эта вспомогательная функция move, где мы получаем текущее направление, и затем на основе этого направления мы просто вычисляем новую позицию новой головы. Так что да, это все, что делается здесь, и теперь давайте кое-что изменим, так что первое, что я хочу, это изменить имя класса на SnakeGameAI, чтобы сделать понятным, что это игра, управляемая агентом. И теперь, первое, что нам нужно, это функциональность reset, так что здесь, я уже имею этот комментарий, где мы инициализируем состояние игры, так что теперь мы хотим отрефакторить все это в функцию reset, поэтому мы создаем новую функцию define, и затем давайте назовем это reset, и она получает только self и никаких других аргументов, и здесь мы можем взять весь этот код, а затем просто вставить его сюда, и в нашем инициализаторе мы затем вызываем self.reset.

Итак, это первое, что нам нужно. Дополнительно мы хотим отслеживать игровую итерацию или итерацию кадра, поэтому давайте назовем это self.frame_iteration и в начале это просто ноль. Затем эта define place_food может оставаться как есть, и теперь нам нужно изменить функцию play_step, так что в первую очередь, если мы посмотрим на обзор здесь, я уже сказал вам, что теперь нам нужно дать это действие от агента, и нам нужно вернуть вознаграждение, так что давайте начнем с использования этого параметра action, и здесь мы получаем пользовательский ввод, так что на самом деле прямо сейчас мы можем избавиться от этого, так что единственное, что мы все еще проверяем, это если мы хотим выйти из игры. И теперь здесь, у нас уже есть эта вспомогательная функция, где мы двигаемся в текущем направлении, так что на самом деле то, что мы изменим здесь сейчас, эта функция move не получает направление от пользовательского ввода, так что теперь здесь она получает действие, и затем мы должны определить новое направление.

Так что мы сделаем это через секунду, но сначала давайте просто изменим это, и затем здесь мы вызываем self.move с action, и затем мы обновляем голову, затем мы проверяем, закончилась ли наша игра или нет, и мы на самом деле теперь нам также нужно вознаграждение, поэтому мы просто говорим reward равняется нулю. И давайте вернемся к слайдам с прошлого раза, так что вознаграждение действительно просто, всякий раз, когда мы съедаем еду, мы говорим плюс 10, когда мы проигрываем или когда мы умираем, тогда мы говорим, что наше вознаграждение равно -10, а для всего остального мы просто остаемся на нуле, поэтому мы инициализируем вознаграждение с нулем, затем, если у нас есть столкновение и игра окончена, тогда мы говорим, что наше вознаграждение равно -10, и мы хотим вернуть это также, поэтому возвращаем вознаграждение, game_over и self.score. И здесь мы проверяем только если у нас есть столкновение, поэтому здесь я на самом деле хочу сделать еще одну проверку, так что если ничего не происходит в течение долгого времени, так что если наша змейка не улучшается и не ест еду, но также и не умирает, тогда мы также хотим проверить это, и если это происходит слишком долго, тогда мы также остановимся здесь, так что мы можем сказать or, а затем здесь мы говорим if self.frame_iteration и если это становится слишком большим без каких-либо событий, тогда мы остановимся здесь.

Так что здесь я использую эту маленькую формулу, если это больше, чем 100, умноженное на длину нашей змейки, так что помните, это список, тогда мы остановимся, так что это тоже так, тогда это зависит от длины змейки, так что чем длиннее наша змейка, тем больше у нее времени, так что, но затем, если она становится больше этого значения, тогда мы остановимся. И конечно мы должны обновить self.frame_iteration, и мы можем просто сделать это здесь в начале, так что для каждого игрового шага мы говорим self.frame_iteration += 1. И когда мы сбрасываем это, тогда мы сбрасываем это обратно к нулю. Так что это здесь, и затем да, если мы остановимся, у нас будет вознаграждение -10.

Затем здесь, если наша голова касается еды, тогда мы едим еду, поэтому наш счет увеличивается, и наше вознаграждение устанавливается в плюс 10. Затем мы размещаем новую еду и говорим, что в противном случае мы удаляем последнюю часть, поэтому мы просто перемещаемся сюда. Затем это может оставаться как есть, функция обновления, и в самом конце мы также хотим вернуть вознаграждение. Затем для функции is_collision нам нужно небольшое изменение, поэтому здесь я проверяю только self.head, но позже, чтобы вычислить состояние или опасность, о которой я вам говорил, так что если у нас есть взгляд на состояние, так что здесь мы вычисляем опасность, так что если мы, например, находимся здесь, в углу, тогда у нас есть опасность справа, поэтому для этого может быть удобно, если мы не используем self.head внутри здесь, но если мы дадим этой функции точку, так что это получает аргумент point, и давайте скажем по умолчанию это None, и затем здесь мы просто проверяем, если точка None, тогда мы устанавливаем point равным self.head. Так что внутри этого, где мы вызываем это без аргумента, это может оставаться как есть, и затем здесь, конечно, мы должны изменить self.head, это теперь наша точка, так что здесь, если мы коснемся угловой точки здесь и точки здесь и точки здесь, тогда у нас есть столкновение, и здесь, если наша точка находится в теле змейки, тогда у нас также есть столкновение, и в противном случае у нас нет столкновения.

Хорошо, функция обновления пользовательского интерфейса может оставаться такой же, и теперь для функции перемещения здесь нам нужно кое-что изменить, так что теперь мы получаем action, и теперь на основе этого действия мы хотим определить следующий ход. Итак, если мы вернемся к слайдам, так что здесь мы разработали action таким образом, что он имеет три значения: 1 0 0 означает, что мы сохраняем текущее направление и идем прямо, 0 1 0 означает, что мы делаем поворот направо, и 0 0 1 означает, что мы делаем поворот налево, так что это зависит от текущего направления, так что если мы идем вправо и делаем поворот направо, то мы идем вниз дальше, если мы идем вниз и делаем поворот направо, то мы идем влево дальше и так далее, а поворот налево – наоборот.

Так что теперь мы хотим определить направление на основе действия, поэтому давайте напишем быстрый комментарий здесь, у нас есть прямо, поворот направо или поворот налево, поэтому, чтобы получить следующее направление, сначала я хочу определить все возможные направления в порядке по часовой стрелке, поэтому мы говорим clockwise equals и затем список, и здесь мы начинаем с direction.RIGHT, так что здесь, помните, для направления мы используем этот enum class, так что это должно быть одно из этих направлений, так что наши направления по часовой стрелке должны начинаться с direction.RIGHT, затем из этого следующее будет direction.DOWN, затем у нас есть direction.LEFT и в конце у нас есть direction.UP, поэтому вправо, вниз, влево, вверх, это по часовой стрелке, и затем, чтобы получить текущее направление или текущий индекс текущего направления, мы говорим index equals, и затем мы можем сказать clockwise.index, и затем индекс self.direction, так что мы уверены, что это должно быть в этом массиве, потому что self_direction должен быть одним из этих enum значений, и затем мы проверяем эти различные возможные состояния, поэтому эти.

Итак, для этого мы можем использовать numpy, и я думаю, что мы должны импортировать numpy сначала как np, и затем мы можем использовать его здесь, мы можем сказать if numpy, и затем мы используем эту функцию array_equal, и затем здесь мы вставляем действие и массив, который мы хотим сравнить, так что если это равно 1 0 0, тогда мы идем прямо или мы сохраняем текущие направления, поэтому мы просто говорим, что наше новое направление равно и затем clockwise индекса, и затем помните, что индекс – это просто индекс текущего направления, поэтому здесь у нас в основном нет изменений. Затем мы говорим l if, если наш массив, если numpy.array_equal, если действие равно 0 1 0, тогда мы делаем поворот направо, так что это означает, что мы идем по часовой стрелке, так что если мы идем вправо, то следующее направление будет вниз, если мы идем вниз, то следующее направление будет влево, и если мы идем влево, то следующее направление будет вверх, так что здесь мы говорим, что index равно, или это наш next_index на самом деле, и здесь мы говорим, что это текущий индекс плюс 1, но затем по модулю 4, так что это означает, что если мы находимся в конце вверх и затем делаем следующий, если у нас есть индекс, так что это индекс 0 1 2 3, и затем, если у нас есть индекс 4, модуль 4 на самом деле опять индекс ноль.

Итак, из этого мы делаем поворот и затем возвращаемся в начало снова, так что это наш поворот направо, так что теперь это наш new_direction, и теперь наше новое направление – clockwise new_index, и затем в противном случае мы можем просто использовать else здесь, и на самом деле изменить это на l if, так что теперь это последний случай, поэтому здесь это должно быть 0 0 1, и если это так, тогда давайте скопируем и вставим это сюда, тогда наш next_index – это текущий индекс минус один по модулю четыре, так что это на самом деле означает, что мы идем против часовой стрелки, поэтому мы делаем поворот налево, поэтому, если мы начнем с права, то следующий ход будет вверх, и затем следующим будет влево, и затем следующим будет вниз, и затем снова вправо и так далее, так что с этим подходом мы не можем сделать поворот на 180 градусов. Так что теперь это наше новое направление, и затем просто мы говорим, что self.direction равно new_direction, и затем мы продолжаем. Так что здесь мы извлекаем голову, и затем здесь мы должны проверить, если self.direction теперь равно RIGHT, тогда мы увеличиваем позицию x и так далее, если мы имеем левое направление, тогда мы уменьшаем x, и если мы идем вниз, тогда мы на самом деле увеличиваем y, поэтому y начинается вверху на нуле и затем увеличивается, если мы идем вниз, поэтому, если мы идем вниз, тогда мы должны увеличить y, и если мы идем вверх, тогда мы должны уменьшить y, поэтому если self.direction равно UP, тогда y -= block_size.

И кстати, block_size – это просто здесь константа в значении 10, так что это размер одного блока змейки в пикселях. Так что да, это все, что нам нужно здесь в функции перемещения, и теперь здесь нам это больше не нужно, так что это на самом деле больше не работает с пользовательским вводом, так что вы можете просто удалить это, и затем позже мы контролируем этот класс от агента и вызываем эту функцию play_step. Так что да, на данный момент это все, что нам нужно для реализации игры.

Перевод: Часть 3: Реализация агента для управления игрой

Итак, я уже говорил о теории глубокого Q-обучения в первой части, а в прошлой части мы реализовали Pygame, чтобы мы могли использовать его в нашей среде, управляемой агентом, и теперь нам нужен агент. Итак, давайте начнем.

Итак, здесь, если вы не смотрели первые две части, то я настоятельно рекомендую это сделать. Итак, это отправная точка с прошлого раза, и я на самом деле хочу внести еще одно изменение, которое я забыл, поэтому здесь функция is_collision должна на самом деле быть публичной, потому что тогда наш агент должен использовать ее, поэтому просто удалите подчеркивание здесь, и затем также удалите его в самом классе, когда мы вызываем это, так что тогда у нас есть наша игра "Змейка", и я также хочу переименовать это просто в game.

И теперь мы создаем новый файл agent.py, и затем начнем реализацию этого. Итак, сначала здесь мы импортируем torch из PyTorch, затем мы импортируем random, потому что когда позже нам понадобится это, затем нам также нужен импорт numpy как np, и из нашего реализованного класса нам нужна игра "Змейка", поэтому мы говорим из game import SnakeLike, SnakeGameAI.

Итак, я думаю, что это то, как мы называем этот класс SnakeGameAI, так что да, это правильное имя, затем мы также здесь в начале определили этот enum для направления и этот именованный кортеж для точки, который имеет атрибуты x и y, поэтому мы также хотим импортировать эти две вещи, поэтому мы импортируем direction, и мы импортируем Point, и затем мы также говорим из collections, мы хотим импортировать deque, так что это структура данных, где мы хотим хранить наши воспоминания, так что если вы не знаете, что такое дек, то я помещу ссылку в описание ниже, так что это действительно удобно в этом случае, и вы увидите, почему это так позже.

И затем здесь я хочу определить некоторые параметры в качестве констант, поэтому у нас есть максимальная память, скажем, 100 000, поэтому мы можем хранить 100 000 элементов в этой памяти, затем мы также хотим использовать размер пакета, который вы увидите позже, и здесь я установлю это в 1000, так что вы можете поиграть с этими параметрами, и я также хочу иметь скорость обучения позже, и я хочу установить это в 0 0 1, и да, не стесняйтесь изменить это, и затем мы начинаем создавать наш класс Agent, и он, конечно, получает функцию init с self и никакими другими аргументами. И затем давайте взглянем на слайды из первой части, где я объяснял обучение, поэтому мы хотим создать функцию обучения, где мы делаем все это, поэтому нам нужно получить состояние, вычислить состояние, где мы осведомлены о текущей среде, затем нам нужно вычислить следующий ход из состояния, и нам нужно затем мы хотим обновить или сделать следующий шаг и вызвать game.play_step, и затем вычислить новое состояние снова.

Затем мы хотим сохранить все в памяти, и затем мы также хотим обучить нашу модель, поэтому нам нужно сохранить игру и модель в этом классе, поэтому, прежде всего, позвольте мне создать функции, которые нам нужны сначала, поэтому нам нужна функция get_state, которая получает self, и это получает игру, и затем мы вычисляем состояние, которое я показал вам с этими 11 различными переменными. Затем мы хотим иметь функцию, которую мы вызываем remember,
remember, и она имеет self, и здесь мы хотим поместить состояние, затем действие, затем мы хотим запомнить вознаграждение за это действие, и мы хотим вычислить или мы хотим сохранить следующее состояние, next_state, и мы также хотим сохранить done или бит, или вы можете также назвать это game_over, так что это текущая переменная состояния game_over.

Затем нам нужны две разные функции для обучения, и мы вызываем это define train_on_long_memory, и это нуждается только в self, так что я объясню это позже, и мы также, давайте скопируем и вставим это, у меня также есть функция define train_on_short_memory, так что это только с одним шагом, вы увидите это позже. Затем нам нужна функция, и мы вызываем это get_action, чтобы получить действие на основе состояния, так что она получает self и состояние, и сначала мы просто говорим pass, и это все функции, которые нам нужны, я думаю, и затем я хочу иметь глобальную функцию, которую я называю просто train, и здесь мы говорим pass, и затем, когда мы запускаем этот модуль h и dot pi, поэтому мы говорим if name underscore equals equals main, затем мы просто вызываем эту функцию train, и затем мы можем запустить скрипт, сказав python agent dot pi, как я это делал в самом первом руководстве.

Итак, давайте начнем реализацию агента и функции обучения, поэтому давайте начнем с функции init агента, поэтому здесь что я хочу сохранить, это сначала я хочу сохранить еще несколько параметров, поэтому self.number_of_games, так что я хочу отслеживать это, так что это ноль в начале, затем self.epsilon equals ноль в начале, это параметр для управления случайностью, так что вы увидите это позже. Затем нам также нужно self dot gamma equals zero, так что это так называемая скорость этого дисконтирования, о которой я кратко рассказывал в первом руководстве, я объясню это немного больше в следующем руководстве, где мы реализуем модель и фактический алгоритм глубокого Q-обучения. Затем мы хотим иметь память, поэтому мы говорим self.memory equals и для этого мы используем этот стек, и это может иметь аргумент max leng equals и здесь мы говорим max_memory, и что тогда произойдет, если мы превысим эту память, то он автоматически удалит элементы слева, поэтому тогда он вызовет pop_left для нас, и поэтому этот deque действительно удобен здесь. И затем позже здесь мы также хотим иметь нашу модель и тренера, поэтому я оставлю это как to-do для последней части в следующем видео.

И теперь это все для функции init, и теперь мы можем вернуться и теперь давайте сделаем функцию обучения следующей, поэтому снова давайте взглянем на эти слайды, поэтому нам нужны эти функции в этом порядке. Итак, давайте сначала напишем некоторые комментарии, поэтому сначала давайте создадим некоторые списки для отслеживания очков, так что это пустой список в начале, и это используется для построения графиков позже.

Итак, затем мы также хотим отслеживать средние баллы или средние баллы, это также пустой список в начале, затем наш total_score равен нулю в начале, наша запись, наш лучший счет – это ноль в начале. Затем мы настраиваем агента, поэтому agent equals an agent, и нам также нужно создать игру, поэтому игра – это объект SnakeGameAI, и затем здесь мы создаем наш цикл обучения, поэтому мы говорим while true, так что это должно в основном запускаться навсегда, пока мы не выйдем из скрипта. И теперь здесь давайте напишем некоторые комментарии, поэтому мы хотим получить старое состояние или текущее состояние, поэтому здесь давайте скажем state_old equals, и затем мы вызываем agent dot get states, и это получает игру, поэтому мы уже настроили это правильно, мы только должны реализовать это. Затем, после этого, мы хотим получить ход на основе этого текущего состояния, поэтому мы говорим, что the final move equals agent dot get_action, так что мы на самом деле назвали это action, и действие основано на состоянии.

Затем, с этим ходом, мы хотим выполнить ход и затем получить новое состояние, поэтому для этого мы говорим reward, done и score равны, и здесь мы вызываем game dot play_step с прошлого раза, поэтому я думаю, что game dot play_step с действием, да, game dot play_step, и это получает the final move, и затем мы получаем state_old, или новое, теперь новое состояние, state new equals agent и снова get state теперь с новой игрой. Затем после этого мы хотим обучить короткую память агента, поэтому только для одного шага, поэтому для этого мы говорим agent agent dot train short memory, и это получает, если у нас есть взгляд здесь. На самом деле эта короткая память должна получать некоторые параметры, поэтому именно те же, которые мы поместили в функцию remember, поэтому train short memory получает все эти переменные, и затем здесь, когда мы вызываем это, теперь мы должны получить некоторые подсказки штамм, или давайте сохраним этот файл, и затем скажем agent dot train short memory, и теперь мы должны получить подсказки, нет, мы не получаем этого, но на самом деле мы хотим иметь the state action reward next state и done.

Итак, здесь давайте сделаем это, так что скажем, давайте скажем state_old, затем действие, которое было the final move, затем вознаграждение, затем the state new и в качестве последнего, состояние done или переменная состояния game_over. Итак, теперь у нас есть это, затем мы хотим запомнить все это и сохранить это в памяти, поэтому мы говорим agent dot remember, и затем здесь это получает те же переменные, поэтому мы хотим сохранить все это в нашем deque, и затем это все, что нам нужно. Итак, теперь мы проверяем, если done или если game over, тогда, если это верно, тогда что мы хотим сделать, это мы хотим, давайте напишем комментарий обучить длинную память, и это также называется replay memory или experience replay, и это очень важно для нашего агента, поэтому теперь он снова тренируется на всех предыдущих ходах и играх, в которые он играл, и это чрезвычайно помогает ему улучшиться. И мы также здесь хотим строить графики результатов, поэтому, прежде всего, мы хотим перезапустить игру, поэтому мы можем просто сделать это, сказав game dot reset, у нас уже есть эта функция здесь, поэтому это инициализирует состояние игры и сбрасывает все, поэтому счет, змея, итерация кадра и размещает начальную змею и еду, поэтому теперь у нас есть это. Затем мы хотим увеличить agents dot number_of_games, поэтому это плюс равно один, затем мы хотим сказать agent dot train long memory, и это не нуждается ни в каких аргументах. Затем мы хотим проверить, есть ли у нас новый рекорд, поэтому, если счет больше, чем текущая запись, тогда мы говорим, что запись равна нашему новому счету, и мы также хотим оставить это как to-do здесь, поэтому здесь мы хотим сказать agent dot model dot save позже, когда у нас будет модель.

И поэтому здесь, в этом, мы хотим сохранить это как self.model, и теперь, что мы также хотим сделать здесь, давайте распечатаем некоторую информацию, поэтому распечатать игру, а затем текущий номер и затем счет и запись. Итак, здесь давайте скажем, что наша игра – это agent dot n games, затем мы также хотим построить график или распечатать счет, так что это просто счет, и мы хотим распечатать текущую запись, так что запись равна запись, и затем здесь мы хотим сделать построение графиков, поэтому я реализую это в следующем руководстве, поэтому я оставлю это как to-do. Итак, это все для нашей функции обучения, поэтому то, что я показал на слайдах, и теперь, конечно, мы должны реализовать эти функции, поэтому для функции get_state, давайте вернемся к этому обзору, и здесь, как я сказал, мы храним 11 значений, поэтому, если опасность прямо, справа или слева, затем текущее направление, поэтому только одно из них равно единице, и затем положение еды, если это слева от змеи, справа от змеи, вверх или вниз от змеи, поэтому это 11 состояний.

И теперь позвольте мне на самом деле скопировать и вставить код сюда, чтобы я не совершил никаких ошибок, но мы рассмотрим это, поэтому сначала давайте возьмем голову из этой игры, поэтому мы можем сделать это, вызвав game dot snake zero, так что это список, и первый элемент – это наша голова. Затем, давайте создадим некоторые точки рядом с этой головой во всех направлениях, которые нам нужно проверить, если это касается границы, и если это опасность, поэтому для этого мы можем использовать этот именованный кортеж, поэтому мы можем создать точку с этим местоположением, но минус 20, поэтому 20 жестко закодировано здесь, поэтому это число, которое я использовал для размера блока, поэтому, как это мы создаем четыре точки вокруг головы. Затем текущее направление – это просто логическое значение, где мы проверяем, если текущее направление игры равно одному из этих, поэтому только одно из этих равно одному, а другое равно нулю или false.

И затем мы создаем этот массив или этот список с этими 11 состояниями, поэтому здесь мы проверяем, если опасность прямо или впереди, и это зависит от текущего направления, поэтому, если мы идем вправо, и точка справа от нас дает нам столкновение, тогда у нас есть опасность, то же самое, или если мы идем влево, и наша левая точка вызывает столкновение, тогда у нас также есть опасность здесь и так далее, поэтому это опасно прямо, и затем опасность справа означает, что, если мы идем вверх, и точка справа от нас даст столкновение, тогда у нас есть опасность для поворота направо в основном, и так далее, и то же самое для левого, поэтому это может быть немного сложно, поэтому я рекомендую, чтобы вы остановились здесь и рассмотрели эту логику для себя снова. Итак, да, эти только имеют дать нам три значения в нашем состоянии до сих пор, затем у нас есть направление движения, где только одно из них верно, а другое – ложно. И для местоположения еды мы просто проверяем, если еда игры x меньше, чем голова игры x, тогда у нас есть еда слева от нас, и таким же образом мы проверяем для вправо, вверх и вниз, и затем мы преобразуем наш список в массив numpy и говорим, что тип данных – это в, так что это хороший маленький трюк, чтобы преобразовать эти true или false boolean значения в ноль или один.

Итак, да, теперь это метод get_state. Теперь давайте перейдем к функции remember, поэтому здесь мы хотим запомнить все это в нашей памяти, поэтому это deque, и это очень просто, поэтому здесь мы говорим self dot memory, и затем deque также имеет функцию append, где мы хотим добавить все это в этом порядке, поэтому состояние, действие, вознаграждение, следующее состояние и состояние game_over, и, как я уже сказал, если это превышает максимальную память, затем pop_left if max mem memory достигнут, и да, это функция remember. Затем давайте начнем реализацию функций train_long и short memory, поэтому для этого, так что на самом деле мы храним модель и тренера здесь, поэтому давайте на самом деле скажем, что self dot model equals, давайте скажем, что это только None в начале и оставим to-do, и self dot trainer equals None в начале, и это to-do, поэтому это объекты, которые мы создаем в следующем руководстве. И затем здесь мы вызываем этого тренера, чтобы фактически сделать шаг оптимизации, поэтому давайте начнем здесь, поэтому только для одного шага мы говорим self.trainer, и затем это должно получить функцию, которую мы вызываем, давайте назовем это train step, и затем это получает все эти переменные, поэтому состояние, действие, вознаграждение, следующее состояние и состояние game_over, и это все, что нам нужно, чтобы обучить это только для одного шага игры, и мы разрабатываем эту функцию, чтобы она принимала либо только одно состояние, как это, но она также может принимать целый тензор или массив numpy и затем использовать несколько в качестве так называемого пакета, поэтому давайте сделаем это здесь, поэтому для этого мы берем переменные из нашей памяти, поэтому здесь мы хотим взять пакет, и поэтому в начале мы определили размер пакета 1 000, поэтому мы хотим взять 1 000 образцов из нашей памяти, но сначала мы проверяем, если мы уже имеем тысячу образцов в нашей памяти, поэтому мы говорим if lang и self dot memory, если это меньше, чем размер пакета, тогда мы просто, или на самом деле, давайте скажем, если это больше, поэтому, если это больше, мы хотим иметь случайный образец и сказать, что mini sample equals, и затем мы хотим получить случайный образец, поэтому мы можем использовать random dot sample, поэтому мы уже импортировали модуль random.

random dot sample из self dot memory, и в качестве размера он должен иметь размер пакета, так что это вернет список кортежей, и это потому, что здесь я забыл одну важную вещь, поэтому, когда мы хотим сохранить это и добавить это, мы хотим добавить это только как один элемент, поэтому только один кортеж, поэтому нам нужны дополнительные скобки здесь, поэтому это один кортеж, который мы храним, и затем здесь мы получаем размер пакета количество кортежей, и в противном случае, else, если у нас нет тысячи элементов еще, тогда мы просто берем всю память, поэтому мы говорим mini sample equals self dot memory, и затем мы снова хотим вызвать этот шаг обучения, и для этого, поэтому давайте вызовем это здесь снова self.trainer.trainstep, но здесь у нас есть несколько состояний, поэтому давайте назовем это состояния, действия, вознаграждения, следующие состояния и done, и прямо сейчас, поэтому теперь у нас есть это в этом формате, что у нас есть один кортеж за другим, и теперь мы хотим извлечь это из мини-образца, и затем поместить каждый states вместе, каждый action вместе, каждый reward вместе и так далее, и это на самом деле действительно просто с помощью python, поэтому мы можем сказать, что мы хотим извлечь states, actions, rewards, next states и duns game overs, и здесь мы просто используем встроенную функцию zip, и должны использовать одну звездочку, и затем аргумент мини-образца, поэтому да, проверьте это для себя, если вы не знаете, как работает функция zip, но опять же, теперь она помещает каждый states вместе, каждый actions и так далее, если это слишком сложно для вас, тогда вы можете также просто сделать цикл for, поэтому вы можете перебрать это мини-образец и в основном сказать для action или для state action rewards next state и done в одном мини-образце, и затем снова вы вызываете это здесь только для одного, только для одного аргумента. Так что да, вы можете сделать это обоими способами, но на самом деле я рекомендую сделать это этим способом, потому что тогда у вас есть это только как один аргумент, и затем вы можете сделать это быстрее в pytorch. Хорошо, теперь у нас есть обе функции обучения, теперь нам нужна только функция get_action, поэтому здесь в начале мы хотим сделать некоторые случайные ходы, и это также называется компромиссом между разведкой и эксплуатацией в глубоком обучении, поэтому в какой-то момент или в начале мы хотим убедиться, что мы также делаем случайные ходы и исследуем окружающую среду, но затем, чем лучше наша модель или наш агент становится, тем меньше случайных ходов мы хотим иметь, и тем больше мы хотим использовать нашего агента или нашу модель.

Итак, да, это то, что мы хотим сделать здесь, поэтому для этого мы используем этот параметр epsilon, который мы инициализировали в начале, поэтому для этого давайте реализуем это сначала, поэтому мы говорим self dot epsilon equals, и это зависит от количества игр, поэтому здесь я жестко кодирую это в 80 минус self dot number of games, вы можете поиграть с этим. И затем давайте получим окончательный ход, поэтому в начале мы говорим 0 0 0, и затем одно из них теперь должно быть верным, поэтому здесь сначала давайте проверим, если random dot rent int и здесь между 0 и 200, если это меньше, чем self dot epsilon, тогда мы делаем случайный ход, поэтому мы говорим, что ход равен random dot rant ins, и это должно быть между 0 и 2, поэтому 2 на самом деле включен здесь, и это даст нам случайное значение 0 1 или 2, и теперь этот индекс должен быть установлен в один, поэтому мы говорим, что final move этого индекса move равен одному, и да, поэтому, чем больше игр у нас есть, тем меньше наш epsilon будет получить, и чем меньше epsilon получит, тем меньше частоты это будет меньше, чем epsilon, и когда это даже, это может даже стать отрицательным, и тогда у нас больше нет случайного хода.

Итак, опять же, если это было слишком быстро здесь, то не стесняйтесь приостановить и подумать об этой логике снова. Итак, теперь у нас есть это, и в противном случае, else, поэтому здесь мы на самом деле здесь мы хотим сделать ход, который основан на нашей модели, поэтому мы хотим получить прогноз, прогноз равен self dot model dot predict, и он хочет предсказать действие на основе одного состояния, поэтому мы вызываем состояние ноль, и мы получаем это здесь, но мы хотим преобразовать это в тензор, поэтому мы говорим state 0 equals torch dot tensor, и в качестве входа он получает состояние, и мы также даем ему тип данных равен, давайте используем torch dot float здесь. Затем мы вызываем self.model predict с состоянием, это даст нам прогноз, и это может быть необработанным значением, поэтому, если мы вернемся к этому слайду, так что это может быть необработанным значением, и затем мы берем максимум из этого и устанавливаем этот индекс в a1. Итак, здесь мы говорим, что наш ход равен, и мы получаем это, сказав torch arc max, и arc max прогноза, и это тензор снова, и чтобы преобразовать это только в одно число, мы можем вызвать элемент. И теперь это целое число, и теперь опять же мы можем сказать, что final move smooth index равен одному.

И теперь у нас есть это, поэтому теперь мы возвращаем окончательный ход здесь. Итак, да, это все, что нам нужно. Итак, теперь у нас есть это, и мы можем сохранить это так, и теперь у нас есть все, что нам нужно для нашего класса агента, и теперь в следующем, что мы должны сделать здесь, это реализовать модель и тренера, а затем также построение графиков.

Перевод: Часть 4: Создание и обучение нейронной сети

Итак, давайте вернемся к коду, и здесь я оставил это essay to do, поэтому нам нужна модель и тренер, поэтому давайте создадим новый файл, и давайте назовем это model dot pi, и затем здесь давайте сначала импортируем все вещи, которые нам нужны, поэтому нам нужно import torch, затем мы хотим импортировать torch dot n n s n n, затем мы хотим импортировать torch dot optim s optim и также импортировать torch dot n n dot functional s capital f, и мы также хотим импортировать o s для сохранения нашей модели. И теперь мы хотим реализовать два класса, один для модели и один для тренера, поэтому давайте создадим класс, и давайте назовем это linear_underscore_qnet, и это должно унаследовать от nn dot module.

И, кстати, если вам не комфортно с pytorch и хотите узнать, как использовать эту платформу, то у меня есть серия для начинающих здесь, в этом руководстве бесплатно, и я помещу ссылку в описание, так что это научит вас всему, чтобы начать работу с pytorch. Итак, прямо сейчас давайте начнем реализацию этой линейной функции qnet, поэтому нам нужна функция init define init, и нам нужно иметь self, и это получает размер входа, размер входа, скрытый размер и размер выхода.

И затем первое, что мы хотим сделать, это вызвать этот супер инициализатор, поэтому мы вызываем super init, и здесь это очень просто, поэтому, если мы посмотрим на слайды, то наши модели должны быть просто прямой нейронной сетью со входным слоем, скрытым слоем и выходным слоем. Не стесняйтесь расширить это и улучшить это, но это работает хорошо для этого случая, и это на самом деле не так уж и плохо здесь, поэтому давайте создадим два линейных слоя, поэтому давайте назовем это self.linear1 equals nn.linear, и это получает размер входа в качестве входа и затем скрытый размер в качестве выходного размера. Затем у нас есть self.linear2 equals nn.linear, и теперь это получает скрытый размер в качестве входа и размер выхода в качестве выхода.

Затем, как всегда в pi torch, мы должны реализовать функцию forward с self, и это получает x, поэтому тензор, и здесь что мы хотим сделать, это сначала мы хотим применить линейный слой, и мы также используем функцию актуации здесь, поэтому опять же, если вы не знаете, что это такое, то проверьте мою серию для начинающих, там я объясняю все это, поэтому мы говорим x, и затем мы можем вызвать f dot reloose, мы используем это непосредственно из функционального модуля, и здесь мы говорим self dot linear one с нашим тензором x в качестве входа, поэтому сначала мы делаем линейный слой, затем мы применяем функцию актуации, и затем опять же мы применяем второй слой, поэтому мы вызываем self dot linear 2 с x, и нам не нужна функция актуации здесь в конце, мы можем просто использовать необработанные числа и вернуть x. Итак, это наша функция forward. Затем давайте также реализуем вспомогательную функцию для сохранения модели позже, поэтому давайте назовем это self safe, и это получает имя файла в качестве входа, и я использую значение по умолчанию здесь, поэтому model dot pth – это просто имя файла, и затем в последний раз я думаю, что я уже назвал эту функцию, поэтому нет, но теперь мы можем закомментировать это, поэтому, если у нас есть новый высокий балл, тогда мы вызываем agent dot model dot save, и здесь давайте создадим новую папку здесь, поэтому давайте скажем, что это путь к папке модели равен, и давайте создадим новую папку в текущем каталоге и назовем это модель, поэтому dot slash model, и затем мы проверяем, если это уже существует, поэтому файл в этой папке, поэтому мы можем сказать if not os dot path dot exists, и затем мы говорим, что наш путь к папке модели, затем мы создаем это, поэтому мы говорим os dot makers, и мы хотим сделать этот путь к папке модели. Затем мы создаем это окончательное имя файла, поэтому мы говорим, что имя файла равно os dot path dot join, и мы хотим объединить путь к папке модели и имя файла, которое мы используем здесь в качестве входа.

Теперь это имя файла для сохранения, и затем мы хотим сохранить это, и мы можем сделать это с torch dot save, и мы хотим сохранить self dot state dict, поэтому у меня также есть учебник о сохранении модели, нам нужно только сохранить этот словарь состояния, и затем в качестве пути мы используем это имя файла. Итак, теперь это все, что нам нужно для нашего линейного q net, и теперь, чтобы сделать фактическое обучение и оптимизацию, мы также делаем это в классе, который я называю q trainer, и теперь здесь что мы хотим сделать, мы хотим реализовать функцию init, которая получает self, затем это также получает модель, затем это должно получить скорость обучения, и это должно получить гамма-параметр, и здесь мы просто сохраняем все self.lr equals lr self dot gamma equals gamma, и мы также храним модель, поэтому мы говорим self dot model equals model. Затем, чтобы сделать шаг оптимизации pie charge, нам нужен оптимизатор, поэтому мы можем создать это, вызвав self.optim, или давайте назовем это optimizer equals, и мы получаем это из opt-in модуля, и здесь вы можете выбрать один оптимизатор, поэтому я использую оптимизатор atom, и мы хотим оптимизировать model.parameters, и это функция, и затем это также нуждается в скорости обучения, поэтому lr equals self dot l r.

И теперь нам также нужен критерий или функция потерь, поэтому давайте назовем это self dot criterion equals, и теперь, если мы вернемся к этим слайдам в самом конце, мы узнали в первой части, что это не что иное, как среднеквадратическая ошибка, поэтому это очень просто, поэтому мы можем создать это здесь, сказав self.criterion equals, поэтому это nn.mse loss, и теперь это то, что нам нужно в нашем инициализаторе, и затем нам также нужно определить, мы называем это функцией шага обучения, которая получает self, и затем она должна иметь все сохраненные параметры с прошлого раза, поэтому она должна иметь, или давайте взглянем на это, поэтому здесь, когда мы вызываем это, это нужно состояние, окончательный ход, вознаграждение, новые состояния и done, поэтому давайте скопируем и вставим это сюда, и переименуем это слегка, поэтому это просто состояние, это действие, это вознаграждение.

Итак, это новое состояние, это может, давайте назовем это next state здесь, и затем done может остаться как есть, и на данный момент давайте просто сделаем pass здесь. И прежде чем мы реализуем это, давайте вернемся к агенту, и теперь настроим это, поэтому здесь мы говорим from, и мы называем это модель, и мы хотим импортировать линейное, я думаю, что мы называем это linear q net и q trainer. И затем здесь, в инициализаторе, мы хотим создать экземпляр модели и тренера, поэтому self.model equals наш линейный qnet, и теперь это нуждается в размере входа, скрытом размере и размере выхода, поэтому здесь я использую 11 256 и три, поэтому помните, если мы посмотрим и вернемся к слайдам снова, то первое – это размер состояния, поэтому это 11 значений, и вывод должен быть три, потому что и у нас есть три разных, три разных числа в нашем действии.

И вы можете поиграть с этим скрытым размером, но другие должны быть одиннадцать и три. Итак, это модель, и тренер равен q тренеру, и это получает модель, поэтому self.model, затем это получает скорость обучения, равную скорости обучения, которую мы указали здесь, и мы также передаем значение гамма, поэтому гамма равна self dot gamma, и гамма – это скорость дисконтирования, поэтому это должно быть значение, которое меньше 1, и обычно это около 0.8 или 0.9, поэтому в этом случае давайте установим это на 0.9, поэтому вы можете поиграть с этим, а также, но имейте в виду, что это должно быть меньше одного. Итак, теперь у нас есть это, и затем я сделал одну ошибку в последнем руководстве, поэтому это очень важно, чтобы мы исправили это прямо сейчас, поэтому здесь, в функции get action, я на самом деле назвал это self.model predict, но на самом деле pythog не имеет функции predict, поэтому это было бы API для tensorflow, например, поэтому в pi torch мы просто вызываем self.model как это, и затем это выполнит эту функцию forward, поэтому это на самом деле прогноз, тогда поэтому да, пожалуйста, убедитесь, что исправили это.

Окей, теперь у нас есть все, и если мы посмотрим и вернемся назад, то мы видим, что мы вызываем эту функцию self.trainer train step только с одним из этих параметров или для целого пакета, и теперь эта функция может обрабатывать разные размеры. И теперь единственное, что осталось сделать здесь, это на самом деле построить графики результатов, поэтому для этого давайте создадим новый файл, и давайте назовем это hell helper dot pi, и затем здесь позвольте мне на самом деле скопировать и вставить это сюда, поэтому это просто простая функция с matplotlib и i python, и да, здесь мы хотим построить графики счетов, поэтому это список, и мы хотим построить графики в построить средний счет, поэтому давайте создадим их, поэтому здесь в агенте мы говорим из helper, импортировать функцию графика.

И затем внизу здесь в функции обучения, поэтому мы уже создали пустой список для счетов и для средних счетов, и теперь после каждой игры мы хотим добавить счет, поэтому давайте удалим to-do и реализуем это, поэтому мы говорим plot scores dot appends, и затем текущий счет. Затем давайте вычислим новый средний или средний счет, поэтому для этого давайте скажем, что общий счет плюс равен счет, и затем давайте назовем это средний счет равен общий счет, разделенный на количество игр, поэтому agent и игры, и затем мы добавляем это для построения графиков, mean scores dot append средний счет, и затем мы просто вызываем функцию plot с plot scores и затем plot mean scores, и теперь давайте сохраним этот файл, и также давайте сохраним этот файл, и затем давайте попробуем это. Итак, в терминале давайте вызовем agent dot pi, и давайте скрестим пальцы, поэтому у нас есть ошибка синтаксиса в файле model.pi, поэтому здесь у нас на самом деле здесь у нас есть два знака равенства, поэтому давайте исправим это и сохраним это и запустим это снова, и затем мы сделали другую ошибку, ошибка имени, поэтому здесь это на самом деле называется prediction.clone, поэтому опять же давайте сохраним это и запустим это, и теперь он начинает обучение без сбоев, и он также строит графики, поэтому давайте позволим этому запуститься и посмотрим, улучшается ли это.

 
 
Хорошо, как мы видим, алгоритм работает, и змея становится все лучше и лучше, и баллы становятся все выше и выше, и также средний или средний счет становится выше, поэтому я забыл одну важную вещь, которую я покажу вам через секунду, но на данный момент змея не идеальна, и основные проблемы в том, что она захватывает себя иногда, и также иногда она застревает в бесконечной последовательности цикла.

Итак, это то, что вы можете улучшить в качестве домашнего задания, поэтому да, как это теперь она захватила себя, поэтому да, позвольте мне остановить это на самом деле, а затем покажите вам, что я забыл, поэтому в игре мы можем на самом деле установить скорость, поэтому для игры с управлением человеком, когда я хочу играть в это, я установил это в 20, но теперь я рекомендую установить это в большее число, чтобы обучение было быстрее, поэтому, например, вы можете использовать 40 здесь или даже выше, поэтому я иду с 40, и да, я думаю, что это весь код, вы также можете найти это на github, и да, я надеюсь, что вы действительно наслаждались этой маленькой серией об обучении с подкреплением, и если вам понравилось это, то, пожалуйста, нажмите кнопку нравится и рассмотрите возможность подписки на канал, и тогда я надеюсь увидеть вас в следующий раз, пока.